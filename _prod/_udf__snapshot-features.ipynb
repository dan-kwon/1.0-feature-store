{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c206f227-71f4-40dc-92b9-068b306dd674",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "today = datetime.today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15618c0f-2f28-4bef-8d51-acabab026d2b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get longest streak\n",
    "def get_longest_streak(horizon_weeks, snapshot_date):\n",
    "  query = f\"\"\"\n",
    "    with content_consumption as (\n",
    "        select distinct hs_user_id, dt\n",
    "        from silver.fact_content_consumption\n",
    "        where dt between date_add('{snapshot_date}',-7*{horizon_weeks} - 1) and date_add('{snapshot_date}',-1)\n",
    "          and hs_user_id is not null\n",
    "          and upper(hs_user_id) like 'HSUSER%'\n",
    "    ),\n",
    "    content_consumption_w_lag as (\n",
    "        select hs_user_id,\n",
    "               dt,\n",
    "               case when dt = date_add(lag(dt) over(partition by hs_user_id order by dt),1) \n",
    "                    then 0\n",
    "                    else 1\n",
    "                    end new_streak\n",
    "        from content_consumption\n",
    "    ),\n",
    "    streaks as (\n",
    "        select s.*,\n",
    "               sum(new_streak) over(partition by hs_user_id order by dt) streak_no\n",
    "        from content_consumption_w_lag s\n",
    "    ), \n",
    "    all_streaks as (\n",
    "        select hs_user_id,\n",
    "               streak_no,\n",
    "               count(distinct dt) streak_len\n",
    "        from streaks\n",
    "        group by 1,2\n",
    "    )\n",
    "    select hs_user_id,\n",
    "           '{snapshot_date}' snapshot_date,\n",
    "           max(streak_len) longest_streak_last{horizon_weeks}weeks\n",
    "    from all_streaks\n",
    "    group by 1,2\n",
    "    \"\"\"\n",
    "  return spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83ec418f-8e60-42c1-bc2e-1bc11dd8a9d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get days since last playback\n",
    "def get_days_since_last_playback(horizon_weeks, snapshot_date):\n",
    "  query = f\"\"\"\n",
    "    with content_consumption as (\n",
    "        select distinct hs_user_id, dt\n",
    "        from silver.fact_content_consumption\n",
    "        where dt between date_add('{snapshot_date}',-7*{horizon_weeks}-1) and date_add('{snapshot_date}',-1)\n",
    "          and hs_user_id is not null\n",
    "          and upper(hs_user_id) like 'HSUSER%'\n",
    "    )\n",
    "    select hs_user_id,\n",
    "           '{snapshot_date}' snapshot_date,\n",
    "           datediff('{snapshot_date}', max(dt)) days_since_last_playback\n",
    "    from content_consumption\n",
    "    group by 1,2\n",
    "    \"\"\"\n",
    "  return spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da1cb3dd-3540-4009-9051-0e925982658e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# engagement features\n",
    "def get_engagement(horizon_weeks, snapshot_date):\n",
    "  \"\"\"\n",
    "  A function used to pull engagement features for users that exist at a certain snapshot_date\n",
    " \n",
    "    Parameters\n",
    "    ----------\n",
    "    horizon_days : int\n",
    "        cumaltive window of days to aggregate feature over (e.g. total content starts from X days prior to snapshot date to snapshot date)\n",
    "    snapshot : str\n",
    "        date from which to create aggregate windows for features\n",
    "    ...    \n",
    "    Returns\n",
    "    -------\n",
    "    spark dataframe with engagement features, grouped by hs_user_id\n",
    "    \n",
    "  \"\"\"\n",
    "  query = f\"\"\"\n",
    "    with week_range as (\n",
    "      select explode(sequence(1, {horizon_weeks}, 1)) week_number\n",
    "    )\n",
    "    select \n",
    "      f.hs_user_id,\n",
    "      '{snapshot_date}' as snapshot_date,\n",
    "      --'week'||(floor(datediff('{snapshot_date}',f.dt)/7)+1) weeks_before_snapshot,\n",
    "      'last_'||week_number||'w' weeks_before_snapshot,\n",
    "      count(distinct f.content_type) distinct_daily_content_types,\n",
    "      count(distinct f.content_id) distinct_daily_content_titles,\n",
    "      count(distinct f.playlist_id) distinct_daily_playlists,\n",
    "      count(distinct f.device_id) distinct_daily_devices,\n",
    "      count(distinct f.content_start_time) content_starts,\n",
    "      sum(playback_ms)::float/60000 playback_minutes,\n",
    "      sum(duration_ms)::float/60000 duration_minutes,\n",
    "      avg(content_percentage_consumed)::float/100 avg_content_percentage_consumed,\n",
    "      sum(case when content_type = 'wakeup' then playback_ms else 0 end)::float/60000 wakeup_playback_minutes,\n",
    "      sum(case when content_type = 'course' then playback_ms else 0 end)::float/60000 course_playback_minutes,\n",
    "      sum(case when content_type = 'sleepcast' then playback_ms else 0 end)::float/60000 sleepcast_playback_minutes,\n",
    "      sum(case when content_type = 'meditation' then playback_ms else 0 end)::float/60000 meditation_playback_minutes,\n",
    "      sum(case when content_type = 'wind down' then playback_ms else 0 end)::float/60000 winddown_playback_minutes,\n",
    "      sum(case when content_type = 'sleep music' then playback_ms else 0 end)::float/60000 sleepmusic_playback_minutes,\n",
    "      sum(case when content_type = 'mindful activity' then playback_ms else 0 end)::float/60000 mindfulactivity_playback_minutes,\n",
    "      sum(case when content_type = 'soundscape' then playback_ms else 0 end)::float/60000 soundscape_playback_minutes,\n",
    "      sum(case when content_type = 'workout' then playback_ms else 0 end)::float/60000 workout_playback_minutes,\n",
    "      sum(case when content_type = 'focus music' then playback_ms else 0 end)::float/60000 focusmusic_playback_minutes,\n",
    "      sum(case when extract(hour from content_start_time) in (0,1,2,3,22,23) then playback_ms else 0 end)::float/60000 latenight_playback_minutes,\n",
    "      sum(case when extract(hour from content_start_time) in (4,5,6,7,8) then playback_ms else 0 end)::float/60000 earlymorning_playback_minutes,\n",
    "      sum(case when extract(hour from content_start_time) in (9,10,11) then playback_ms else 0 end)::float/60000 latemorning_playback_minutes,\n",
    "      sum(case when extract(hour from content_start_time) in (12,13,14,15,16) then playback_ms else 0 end)::float/60000 afternoon_playback_minutes,\n",
    "      sum(case when extract(hour from content_start_time) in (17,18,19,20,21) then playback_ms else 0 end)::float/60000 evening_playback_minutes\n",
    "    from silver.fact_content_consumption f\n",
    "    right join week_range w\n",
    "      on datediff('{snapshot_date}', f.dt) <= w.week_number*7\n",
    "      and w.week_number in (1,4,8,26,52)\n",
    "    where f.dt between date_add('{snapshot_date}',-7*{horizon_weeks}-1) and date_add('{snapshot_date}',-1)\n",
    "      and f.hs_user_id is not null\n",
    "      and upper(f.hs_user_id) like 'HSUSER%'\n",
    "    group by 1,2,3\"\"\"\n",
    "  return spark.sql(query)\n",
    "\n",
    "# weekly distinct engagement\n",
    "def get_weekly_distinct_engagement(horizon_weeks, snapshot_date):\n",
    "  \"\"\"\n",
    "  A function used to pull engagement features for users that exist at a certain snapshot_date\n",
    " \n",
    "    Parameters\n",
    "    ----------\n",
    "    horizon_days : int\n",
    "        cumaltive window of days to aggregate feature over (e.g. total content starts from X days prior to snapshot date to snapshot date)\n",
    "    snapshot : str\n",
    "        date from which to create aggregate windows for features\n",
    "    ...    \n",
    "    Returns\n",
    "    -------\n",
    "    spark dataframe with engagement features, grouped by hs_user_id\n",
    "    \n",
    "  \"\"\"\n",
    "  \n",
    "  #if horizon_days % 7 == 0:\n",
    "  #  adj_horizon_days = horizon_days\n",
    "  #else:\n",
    "  #  adj_horizon_days = horizon_days + (7 - horizon_days % 7)\n",
    "    \n",
    "  query = f\"\"\"\n",
    "    with week_range as (\n",
    "      select explode(sequence(1, {horizon_weeks}, 1)) week_number\n",
    "    )\n",
    "    select \n",
    "      f.hs_user_id,\n",
    "      '{snapshot_date}' as snapshot_date,\n",
    "      --'week'||(floor(datediff('{snapshot_date}',f.dt)/7)+1) weeks_before_snapshot,\n",
    "      'last_'||week_number||'w' weeks_before_snapshot,\n",
    "      count(distinct f.content_type) distinct_weekly_content_types,\n",
    "      count(distinct f.content_id) distinct_weekly_content_titles,\n",
    "      count(distinct f.playlist_id) distinct_weekly_playlists,\n",
    "      count(distinct f.device_id) distinct_weekly_devices\n",
    "    from silver.fact_content_consumption f\n",
    "    right join week_range w\n",
    "      on datediff('{snapshot_date}', f.dt) <= w.week_number*7\n",
    "      and w.week_number = 52\n",
    "    where f.dt between date_add('{snapshot_date}', -7*{horizon_weeks}-1) and date_add('{snapshot_date}',-1)\n",
    "      and f.hs_user_id is not null\n",
    "      and upper(f.hs_user_id) like 'HSUSER%'\n",
    "    group by 1,2,3\"\"\"\n",
    "  return spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ecb08cf-7ae5-4e8a-8c30-38c3589558ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# event features\n",
    "def get_amplitude_events(horizon_weeks, snapshot_date):\n",
    "  \"\"\"\n",
    "  A function used to pull event features for canceled users\n",
    " \n",
    "    Parameters\n",
    "    ----------\n",
    "    horizon_days : int\n",
    "        number of days before cancel date (e.g. total content starts X days before a user's last day)\n",
    "    paid_subscription_end_date : str\n",
    "        last date of a user's paid subscription\n",
    "    ...    \n",
    "    Returns\n",
    "    -------\n",
    "    spark dataframe with engagement features for every hs_user_id, grouped by days until cancel date\n",
    "    \n",
    "  \"\"\"\n",
    "  query = f\"\"\"\n",
    "    with week_range as (\n",
    "      select explode(sequence(1, {horizon_weeks}, 1)) week_number\n",
    "    ) \n",
    "    select \n",
    "      a.hs_user_id,\n",
    "      '{snapshot_date}' snapshot_date,\n",
    "      'last_'||week_number||'w' weeks_before_snapshot,\n",
    "      count(case when lower(a.event_type) = 'app start' then 1 else null end) app_starts,\n",
    "      count(case when lower(a.event_type) = 'button clickthrough' then 1 else null end) button_clickthroughs,\n",
    "      count(case when lower(a.event_type) = 'subscription amended' then 1 else null end) subscription_amended,\n",
    "      count(case when lower(a.event_type) = 'survey start' then 1 else null end) survey_starts,\n",
    "      count(distinct case when lower(a.event_type) = 'app start' then a.client_dt else null end) active_days_app_start,\n",
    "      count(distinct case when lower(a.event_type) like '%viewed%screenview%' then event_type else null end) distinct_screens_viewed,\n",
    "      count(distinct case when lower(a.event_type) like '%viewed%tab%screenview%' then event_type else null end) distinct_tabs_viewed\n",
    "    from bronze.amplitude a\n",
    "    right join week_range w\n",
    "      on datediff('{snapshot_date}', a.client_dt) <= w.week_number*7\n",
    "      and w.week_number in (1,4,8,26,52)\n",
    "    where a.client_dt between date_add('{snapshot_date}', -7*{horizon_weeks}-1) and date_add('{snapshot_date}',-1)\n",
    "      and a.hs_user_id is not null\n",
    "      and upper(a.hs_user_id) like 'HSUSER%'\n",
    "    group by 1,2,3\"\"\"\n",
    "  return spark.sql(query)\n",
    "\n",
    "def get_distinct_amplitude_events(horizon_weeks, snapshot_date):\n",
    "  \"\"\"\n",
    "  A function used to pull event features for new users\n",
    " \n",
    "    Parameters\n",
    "    ----------\n",
    "    horizon_days : int\n",
    "        number of days before cancel date (e.g. total content starts X days before a user's last day)\n",
    "    paid_subscription_end_date : str\n",
    "        last date of a user's paid subscription\n",
    "    ...    \n",
    "    Returns\n",
    "    -------\n",
    "    spark dataframe with engagement features for every hs_user_id, grouped by days until cancel date\n",
    "    \n",
    "  \"\"\"\n",
    "  query = f\"\"\"\n",
    "    with week_range as (\n",
    "      select explode(sequence(1, {horizon_weeks}, 1)) week_number\n",
    "    ) \n",
    "    select \n",
    "      a.hs_user_id,\n",
    "      '{snapshot_date}' snapshot_date,\n",
    "      --'week'||(floor(datediff('{snapshot_date}', a.client_dt)/7)+1)::string weeks_before_snapshot,\n",
    "      'last_'||week_number||'w' weeks_before_snapshot,\n",
    "      count(distinct case when lower(a.event_type) like '%viewed%screenview%' then event_type else null end) distinct_screens_viewed,\n",
    "      count(distinct case when lower(a.event_type) like '%viewed%tab%screenview%' then event_type else null end) distinct_tabs_viewed\n",
    "    from bronze.amplitude a\n",
    "    right join week_range w\n",
    "      on datediff('{snapshot_date}', a.client_dt) <= w.week_number*7\n",
    "      and w.week_number = 52\n",
    "    where a.client_dt between date_add('{snapshot_date}',-7*{horizon_weeks}-1) and date_add('{snapshot_date}',-1)\n",
    "      and a.hs_user_id is not null\n",
    "      and upper(a.hs_user_id) like 'HSUSER%'\n",
    "    group by 1,2,3\n",
    "  \"\"\"\n",
    "  return spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "666c8558-4b77-46fa-8023-d9dd0aefdd97",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_braze_events(horizon_weeks, snapshot_date):\n",
    "  \"\"\"\n",
    "  A function used to pull event features for canceled users\n",
    " \n",
    "    Parameters\n",
    "    ----------\n",
    "    horizon_days : int\n",
    "        number of days before cancel date (e.g. total content starts X days before a user's last day)\n",
    "    paid_subscription_end_date : str\n",
    "        last date of a user's paid subscription\n",
    "    ...    \n",
    "    Returns\n",
    "    -------\n",
    "    spark dataframe with engagement features for every hs_user_id, grouped by days until cancel date\n",
    "    \n",
    "  \"\"\"\n",
    "  query = f\"\"\"\n",
    "    with week_range as (\n",
    "      select explode(sequence(1, {horizon_weeks}, 1)) week_number\n",
    "    ) \n",
    "    select \n",
    "      b.external_user_id hs_user_id,\n",
    "      '{snapshot_date}' snapshot_date,\n",
    "      'last_'||week_number||'w' weeks_before_snapshot,\n",
    "      count(case when b.event_name = 'users_messages_email_delivery' then 1 else null end) emails_recieved,\n",
    "      count(case when b.event_name = 'users_messages_email_open' then 1 else null end) emails_opened,\n",
    "      count(case when b.event_name = 'users_messages_email_click' then 1 else null end) emails_clicked,\n",
    "      count(case when b.event_name = 'users_messages_email_unsubscribe' then 1 else null end) emails_unsubscribed,\n",
    "      count(case when b.event_name = 'users_messages_pushnotification_send' then 1 else null end) pushnotifications_recieved,\n",
    "      count(case when b.event_name = 'users_messages_pushnotification_open' then 1 else null end) pushnotifications_opened\n",
    "    from bronze.braze_events b\n",
    "    right join week_range w\n",
    "      on datediff('{snapshot_date}', b.dt) <= w.week_number*7\n",
    "      and w.week_number in (1,4,8,26,52)\n",
    "    where b.dt between date_add('{snapshot_date}', -7*{horizon_weeks}-1) and date_add('{snapshot_date}',-1)\n",
    "      and b.external_user_id is not null\n",
    "      and upper(b.external_user_id) like 'HSUSER%'\n",
    "    group by 1,2,3\"\"\"\n",
    "  return spark.sql(query)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "_udf__snapshot-features",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
