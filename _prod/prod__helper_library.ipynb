{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d4237a0-bc44-4f0e-bb41-28697377d024",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "today = datetime.today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4711cf2-35a2-487b-a0fb-50b56d8b069b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# engagement features\n",
    "def get_engagement(horizon, date=today.date()):\n",
    "  \"\"\"\n",
    "  A function used to pull engagement features for existing users\n",
    " \n",
    "    Parameters\n",
    "    ----------\n",
    "    horizon : int\n",
    "        number of days to look back and aggregate (e.g. total content starts in the last X days)\n",
    "    date : str\n",
    "        reference date for feature horizon period, defaults to today's date\n",
    "    ...    \n",
    "    Returns\n",
    "    -------\n",
    "    spark dataframe with engagement features, grouped by hs_user_id and days_from_sign_up\n",
    "  \"\"\"\n",
    "  query = f\"\"\"select \n",
    "                f.hs_user_id,\n",
    "                '{date}' snapshot_date,\n",
    "                count(distinct f.content_type) last{horizon}d_distinct_content_types,\n",
    "                count(distinct f.content_id) last{horizon}d_distinct_content_titles,\n",
    "                count(distinct f.playlist_id) last{horizon}d_distinct_playlists,\n",
    "                count(distinct f.device_id) last{horizon}d_distinct_devices,\n",
    "                count(distinct f.content_start_time) last{horizon}d_content_starts,\n",
    "                sum(playback_ms)::float/60000 last{horizon}d_playback_minutes,\n",
    "                sum(duration_ms)::float/60000 last{horizon}d_duration_minutes,\n",
    "                avg(content_percentage_consumed)::float/100 last{horizon}d_avg_content_percentage_consumed,\n",
    "                (sum(playback_ms)::float/sum(duration_ms)::float) last{horizon}d_content_percentage_consumed,\n",
    "                sum(case when content_type = 'wakeup' then playback_ms else 0 end)::float/60000 last{horizon}d_wakeup_playback_minutes,\n",
    "                sum(case when content_type = 'course' then playback_ms else 0 end)::float/60000 last{horizon}d_course_playback_minutes,\n",
    "                sum(case when content_type = 'sleepcast' then playback_ms else 0 end)::float/60000 last{horizon}d_sleepcast_playback_minutes,\n",
    "                sum(case when content_type = 'meditation' then playback_ms else 0 end)::float/60000 last{horizon}d_meditation_playback_minutes,\n",
    "                sum(case when content_type = 'wind down' then playback_ms else 0 end)::float/60000 last{horizon}d_winddown_playback_minutes,\n",
    "                sum(case when content_type = 'sleep music' then playback_ms else 0 end)::float/60000 last{horizon}d_sleepmusic_playback_minutes,\n",
    "                sum(case when content_type = 'mindful activity' then playback_ms else 0 end)::float/60000 last{horizon}d_mindfulactivity_playback_minutes,\n",
    "                sum(case when content_type = 'soundscape' then playback_ms else 0 end)::float/60000 last{horizon}d_soundscape_playback_minutes,\n",
    "                sum(case when content_type = 'workout' then playback_ms else 0 end)::float/60000 last{horizon}d_workout_playback_minutes,\n",
    "                sum(case when content_type = 'focus music' then playback_ms else 0 end)::float/60000 last{horizon}d_focusmusic_playback_minutes,\n",
    "                sum(case when extract(hour from content_start_time) in (0,1,2,3,22,23) then playback_ms else 0 end)::float/60000 last{horizon}d_latenight_playback_minutes,\n",
    "                sum(case when extract(hour from content_start_time) in (4,5,6,7,8) then playback_ms else 0 end)::float/60000 last{horizon}d_earlymorning_playback_minutes,\n",
    "                sum(case when extract(hour from content_start_time) in (9,10,11) then playback_ms else 0 end)::float/60000 last{horizon}d_latemorning_playback_minutes,\n",
    "                sum(case when extract(hour from content_start_time) in (12,13,14,15,16) then playback_ms else 0 end)::float/60000 last{horizon}d_afternoon_playback_minutes,\n",
    "                sum(case when extract(hour from content_start_time) in (17,18,19,20,21) then playback_ms else 0 end)::float/60000 last{horizon}d_evening_playback_minutes\n",
    "              from silver.fact_content_consumption f\n",
    "              where f.dt between date_sub('{date}',{horizon}) AND date_sub('{date}',1)\n",
    "                and f.hs_user_id is not null\n",
    "              group by 1,2\"\"\"\n",
    "  return spark.sql(query)\n",
    "\n",
    "def get_early_engagement(horizon,date=today.date()-relativedelta(days=28)):\n",
    "  \"\"\"\n",
    "  A function used to pull engagement features for new users\n",
    " \n",
    "    Parameters\n",
    "    ----------\n",
    "    horizon : int\n",
    "        number of days from sign up date (e.g. total content starts X days from sign up)\n",
    "    date : str\n",
    "        first date of month for cohort of interest (e.g. '2021-01-01' would return features for users that signed up from 2021-01-01 to 2021-01-31)\n",
    "    ...    \n",
    "    Returns\n",
    "    -------\n",
    "    spark dataframe with engagement features, grouped by hs_user_id and days_from_sign_up\n",
    "    \n",
    "  \"\"\"\n",
    "  query = f\"\"\"select \n",
    "                f.hs_user_id,\n",
    "                s.dt signup_date,\n",
    "                s.free_trial_start_date,\n",
    "                s.free_trial_end_date,\n",
    "                s.paid_subscription_start_date,\n",
    "                s.paid_subscription_end_date,\n",
    "                'day'||datediff(f.dt, s.dt) days_from_signup,\n",
    "                count(distinct f.content_type) distinct_daily_content_types,\n",
    "                count(distinct f.content_id) distinct_daily_content_titles,\n",
    "                count(distinct f.playlist_id) distinct_daily_playlists,\n",
    "                count(distinct f.device_id) distinct_daily_devices,\n",
    "                count(distinct f.content_start_time) content_starts,\n",
    "                sum(playback_ms)::float/60000 playback_minutes,\n",
    "                sum(duration_ms)::float/60000 duration_minutes,\n",
    "                avg(content_percentage_consumed)::float/100 avg_content_percentage_consumed,\n",
    "                (sum(playback_ms)::float/sum(duration_ms)::float) content_percentage_consumed,\n",
    "                sum(case when content_type = 'wakeup' then playback_ms else 0 end)::float/60000 wakeup_playback_minutes,\n",
    "                sum(case when content_type = 'course' then playback_ms else 0 end)::float/60000 course_playback_minutes,\n",
    "                sum(case when content_type = 'sleepcast' then playback_ms else 0 end)::float/60000 sleepcast_playback_minutes,\n",
    "                sum(case when content_type = 'meditation' then playback_ms else 0 end)::float/60000 meditation_playback_minutes,\n",
    "                sum(case when content_type = 'wind down' then playback_ms else 0 end)::float/60000 winddown_playback_minutes,\n",
    "                sum(case when content_type = 'sleep music' then playback_ms else 0 end)::float/60000 sleepmusic_playback_minutes,\n",
    "                sum(case when content_type = 'mindful activity' then playback_ms else 0 end)::float/60000 mindfulactivity_playback_minutes,\n",
    "                sum(case when content_type = 'soundscape' then playback_ms else 0 end)::float/60000 soundscape_playback_minutes,\n",
    "                sum(case when content_type = 'workout' then playback_ms else 0 end)::float/60000 workout_playback_minutes,\n",
    "                sum(case when content_type = 'focus music' then playback_ms else 0 end)::float/60000 focusmusic_playback_minutes,\n",
    "                sum(case when extract(hour from content_start_time) in (0,1,2,3,22,23) then playback_ms else 0 end)::float/60000 latenight_playback_minutes,\n",
    "                sum(case when extract(hour from content_start_time) in (4,5,6,7,8) then playback_ms else 0 end)::float/60000 earlymorning_playback_minutes,\n",
    "                sum(case when extract(hour from content_start_time) in (9,10,11) then playback_ms else 0 end)::float/60000 latemorning_playback_minutes,\n",
    "                sum(case when extract(hour from content_start_time) in (12,13,14,15,16) then playback_ms else 0 end)::float/60000 afternoon_playback_minutes,\n",
    "                sum(case when extract(hour from content_start_time) in (17,18,19,20,21) then playback_ms else 0 end)::float/60000 evening_playback_minutes\n",
    "              from silver.fact_content_consumption f\n",
    "              inner join silver.fact_subscription s\n",
    "                on f.hs_user_id = s.hs_user_id\n",
    "                and f.dt between s.dt and coalesce(s.paid_subscription_end_date, s.free_trial_end_date)\n",
    "                and s.dt = '{date}'\n",
    "                and datediff(f.dt, s.dt) <= {horizon}\n",
    "              where f.dt::date between '{date}'::date AND '{date}'::date+{horizon}\n",
    "                and f.hs_user_id is not null\n",
    "              GROUP BY 1,2,3,4,5,6,7\"\"\"\n",
    "  return spark.sql(query)\n",
    "\n",
    "def get_early_distinct_engagement(horizon, date=today.date()-relativedelta(days=28)):\n",
    "  \"\"\"\n",
    "  A function used to pull engagement features for new users\n",
    " \n",
    "    Parameters\n",
    "    ----------\n",
    "    horizon : int\n",
    "        number of days from sign up date (e.g. total content starts X days from sign up)\n",
    "    date : str\n",
    "        first date of month for cohort of interest (e.g. '2021-01-01' would return features for users that signed up from 2021-01-01 to 2021-01-31)\n",
    "    ...    \n",
    "    Returns\n",
    "    -------\n",
    "    spark dataframe with engagement features, grouped by hs_user_id and days_from_sign_up\n",
    "    \n",
    "  \"\"\"\n",
    "  query = f\"\"\"select \n",
    "                f.hs_user_id,\n",
    "                s.dt signup_date,\n",
    "                s.free_trial_start_date,\n",
    "                s.free_trial_end_date,\n",
    "                s.paid_subscription_start_date,\n",
    "                s.paid_subscription_end_date,\n",
    "                'week'||(floor(datediff(f.dt, s.dt)/7)+1)::string weeks_from_signup,\n",
    "                count(distinct f.content_type) distinct_weekly_content_types,\n",
    "                count(distinct f.content_id) distinct_weekly_content_titles,\n",
    "                count(distinct f.playlist_id) distinct_weekly_playlists,\n",
    "                count(distinct f.device_id) distinct_weekly_devices\n",
    "              from silver.fact_content_consumption f\n",
    "              inner join silver.fact_subscription s\n",
    "                on f.hs_user_id = s.hs_user_id\n",
    "                and f.dt between s.dt and coalesce(s.paid_subscription_end_date, s.free_trial_end_date)\n",
    "                and s.dt = '{date}'\n",
    "                and datediff(f.dt, s.dt) <= {horizon}\n",
    "              where f.dt::date between '{date}'::date AND '{date}'::date+{horizon}\n",
    "                and f.hs_user_id is not null\n",
    "              GROUP BY 1,2,3,4,5,6,7\"\"\"\n",
    "  return spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "010e2806-917a-4dc4-a528-2cdfa273345b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# event features\n",
    "def get_events(horizon, date=today.date()):\n",
    "  \"\"\"\n",
    "  A function used to pull event features for existing users\n",
    " \n",
    "    Parameters\n",
    "    ----------\n",
    "    horizon : int\n",
    "        number of days to look back and aggregate (e.g. total content starts in the last X days)\n",
    "    date : str\n",
    "        reference date for feature horizon period, defaults to today's date\n",
    "    ...    \n",
    "    Returns\n",
    "    -------\n",
    "    spark dataframe with event features, grouped by hs_user_id and days_from_sign_up\n",
    "  \"\"\"\n",
    "  query = f\"\"\"select \n",
    "                a.hs_user_id,\n",
    "                '{date}' snapshot_date,\n",
    "                count(case when lower(a.event_type) = 'app start' then 1 else null end) last{horizon}d_app_starts,\n",
    "                count(case when lower(a.event_type) = 'email opens' then 1 else null end) last{horizon}d_email_opens,\n",
    "                count(case when lower(a.event_type) = 'email clicks' then 1 else null end) last{horizon}d_email_clicks,\n",
    "                count(case when lower(a.event_type) = 'button clickthrough' then 1 else null end) last{horizon}d_button_clickthroughs,\n",
    "                count(case when lower(a.event_type) = 'subscription amended' then 1 else null end) last{horizon}d_subscription_amended,\n",
    "                count(case when lower(a.event_type) = 'survey start' then 1 else null end) last{horizon}d_survey_starts,\n",
    "                count(distinct case when lower(a.event_type) = 'app start' then a.client_dt else null end) last{horizon}d_active_days_app_start,\n",
    "                count(distinct case when lower(a.event_type) like '%viewed%screenview%' then event_type else null end) last{horizon}d_distinct_screens_viewed,\n",
    "                count(distinct case when lower(a.event_type) like '%viewed%tab%screenview%' then event_type else null end) last{horizon}d_distinct_tabs_viewed\n",
    "              from bronze.amplitude a\n",
    "              where a.client_dt between date_sub('{date}',{horizon}) and date_sub('{date}',1)\n",
    "                and a.hs_user_id is not null\n",
    "              group by 1,2\"\"\"\n",
    "  return spark.sql(query)\n",
    "\n",
    "def get_early_events(horizon,date=today.date()-relativedelta(days=28)):\n",
    "  \"\"\"\n",
    "  A function used to pull event features for new users\n",
    " \n",
    "    Parameters\n",
    "    ----------\n",
    "    horizon : int\n",
    "        number of days from sign up date (e.g. total content starts X days from sign up)\n",
    "    date : str\n",
    "        first date of month for cohort of interest (e.g. '2021-01-01' would return features for users that signed up from 2021-01-01 to 2021-01-31)\n",
    "    ...    \n",
    "    Returns\n",
    "    -------\n",
    "    spark dataframe with event features, grouped by hs_user_id and days_from_sign_up\n",
    "    \n",
    "  \"\"\"\n",
    "  query = f\"\"\"with day_range as (\n",
    "                select explode(sequence(0, {horizon}, 1)) day_number\n",
    "              ) \n",
    "              select \n",
    "                a.hs_user_id,\n",
    "                s.dt signup_date,\n",
    "                s.free_trial_start_date,\n",
    "                s.free_trial_end_date,\n",
    "                s.paid_subscription_start_date,\n",
    "                s.paid_subscription_end_date,\n",
    "                'day'||datediff(a.dt, s.dt) days_from_signup,\n",
    "                count(case when lower(a.event_type) = 'app start' then 1 else null end) app_starts,\n",
    "                count(case when lower(a.event_type) = 'email opens' then 1 else null end) email_opens,\n",
    "                count(case when lower(a.event_type) = 'email clicks' then 1 else null end) email_clicks,\n",
    "                count(case when lower(a.event_type) = 'button clickthrough' then 1 else null end) button_clickthroughs,\n",
    "                count(case when lower(a.event_type) = 'subscription amended' then 1 else null end) subscription_amended,\n",
    "                count(case when lower(a.event_type) = 'survey start' then 1 else null end) survey_starts,\n",
    "                count(distinct case when lower(a.event_type) = 'app start' then a.client_dt else null end) active_days_app_start,\n",
    "                count(distinct case when lower(a.event_type) like '%viewed%screenview%' then event_type else null end) distinct_screens_viewed,\n",
    "                count(distinct case when lower(a.event_type) like '%viewed%tab%screenview%' then event_type else null end) distinct_tabs_viewed\n",
    "              from  bronze.amplitude a\n",
    "              inner join silver.fact_subscription s\n",
    "                on a.hs_user_id = s.hs_user_id\n",
    "                and a.client_dt between s.dt and coalesce(s.paid_subscription_end_date, s.free_trial_end_date)\n",
    "                and s.dt = '{date}'\n",
    "                and datediff(a.dt, s.dt) <= {horizon}\n",
    "              right join day_range d\n",
    "                on datediff(a.client_dt, s.dt) = d.day_number\n",
    "              where a.client_dt::date between '{date}'::date AND '{date}'::date+{horizon}\n",
    "                and a.hs_user_id is not null\n",
    "              group by 1,2,3,4,5,6,7\"\"\"\n",
    "  return spark.sql(query)\n",
    "\n",
    "def get_early_distinct_events(horizon, date=today.date()-relativedelta(days=28)):\n",
    "  \"\"\"\n",
    "  A function used to pull event features for new users\n",
    " \n",
    "    Parameters\n",
    "    ----------\n",
    "    horizon : int\n",
    "        number of days from sign up date (e.g. total content starts X days from sign up)\n",
    "    date : str\n",
    "        first date of month for cohort of interest (e.g. '2021-01-01' would return features for users that signed up from 2021-01-01 to 2021-01-31)\n",
    "    ...    \n",
    "    Returns\n",
    "    -------\n",
    "    spark dataframe with event features, grouped by hs_user_id and days_from_sign_up\n",
    "    \n",
    "  \"\"\"\n",
    "  query = f\"\"\"select \n",
    "                a.hs_user_id,\n",
    "                s.dt signup_date,\n",
    "                s.free_trial_start_date,\n",
    "                s.free_trial_end_date,\n",
    "                s.paid_subscription_start_date,\n",
    "                s.paid_subscription_end_date,\n",
    "                'week'||(floor(datediff(a.dt, s.dt)/7)+1)::string weeks_from_signup,\n",
    "                count(distinct case when lower(a.event_type) like '%viewed%screenview%' then event_type else null end) distinct_screens_viewed,\n",
    "                count(distinct case when lower(a.event_type) like '%viewed%tab%screenview%' then event_type else null end) distinct_tabs_viewed\n",
    "              from bronze.amplitude a\n",
    "              inner join silver.fact_subscription s\n",
    "                on a.hs_user_id = s.hs_user_id\n",
    "                and a.client_dt between s.dt and coalesce(s.paid_subscription_end_date, s.free_trial_end_date)\n",
    "                and s.dt = '{date}'\n",
    "                and datediff(a.dt, s.dt) <= {horizon}\n",
    "              where a.client_dt::date between '{date}'::date AND '{date}'::date+{horizon}\n",
    "                and a.hs_user_id is not null\n",
    "              GROUP BY 1,2,3,4,5,6,7\"\"\"\n",
    "  return spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4dee010b-6c9c-4104-8600-0c57a7da2ec7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# returns specified category that is most recent\n",
    "def get_most_recent_col(col, rolling_days, snapshot_date=today.date()):\n",
    "  query = f\"\"\"SELECT \n",
    "                hs_user_id,\n",
    "                '{snapshot_date}' AS snapshot_date,\n",
    "                {col} as most_recent_{col}\n",
    "              FROM (\n",
    "                SELECT\n",
    "                  hs_user_id, \n",
    "                  {col}, \n",
    "                  dt, \n",
    "                  row_number() over(partition by hs_user_id order by dt desc) as rank\n",
    "                FROM silver.fact_content_consumption f\n",
    "                WHERE f.dt BETWEEN '{snapshot_date}'::DATE-{rolling_days} AND '{snapshot_date}'::DATE-1\n",
    "                  AND hs_user_id is not null\n",
    "                  AND playback_ms > 10000\n",
    "                  )\n",
    "              WHERE rank = 1\"\"\"\n",
    "  return spark.sql(query)\n",
    "\n",
    "# returns specified category that has the most playback in the specified window\n",
    "def get_most_playback_col(col, rolling_days, snapshot_date=today.date()):\n",
    "  query = f\"\"\"SELECT \n",
    "                hs_user_id, \n",
    "                '{snapshot_date}' AS snapshot_date,\n",
    "                {col} as most_playback_{col}\n",
    "              FROM (\n",
    "                SELECT\n",
    "                  hs_user_id, \n",
    "                  {col}, \n",
    "                  row_number() over(partition by hs_user_id order by SUM(playback_ms) desc) AS rank\n",
    "                FROM silver.fact_content_consumption f\n",
    "                WHERE f.dt BETWEEN '{snapshot_date}'::DATE-{rolling_days} AND '{snapshot_date}'::DATE-1\n",
    "                  AND hs_user_id is not null\n",
    "                GROUP BY 1,2\n",
    "              )\n",
    "              WHERE rank = 1\"\"\"\n",
    "  return spark.sql(query)\n",
    "\n",
    "# returns specified category that is the most frequently occuring within the specified window\n",
    "def get_most_freq_col(col, rolling_days, snapshot_date=today.date()):\n",
    "  query = f\"\"\"SELECT \n",
    "                hs_user_id,\n",
    "                '{snapshot_date}' AS snapshot_date,\n",
    "                {col} as most_freq_{col}\n",
    "              FROM (\n",
    "                SELECT\n",
    "                  hs_user_id, \n",
    "                  {col},\n",
    "                  row_number() over(partition by hs_user_id order by COUNT(*) desc) AS rank\n",
    "                FROM silver.fact_content_consumption f\n",
    "                WHERE f.dt BETWEEN '{snapshot_date}'::DATE-{rolling_days} AND '{snapshot_date}'::DATE-1\n",
    "                  AND hs_user_id is not null\n",
    "                  AND playback_ms > 10000\n",
    "                GROUP BY 1,2\n",
    "              )\n",
    "              WHERE rank = 1\"\"\"\n",
    "  return spark.sql(query)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "prod__helper_library",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
